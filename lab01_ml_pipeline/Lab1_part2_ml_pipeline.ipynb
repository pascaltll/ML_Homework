{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-86e0de040aac317a",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "HAI5THit-_QP"
      },
      "source": [
        "# Lab assignment â„–1, part 2\n",
        "\n",
        "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
        "\n",
        "Several comments:\n",
        "* Don't hesitate to ask questions, it's a good practice.\n",
        "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
        "* Blocks of this lab will be graded separately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1LpifrP-_QY"
      },
      "source": [
        "__*This is the second part of the assignment. First and third parts are waiting for you in the same directory.*__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-512ba712fc0fc065",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "hqmFJAM8-_Qa"
      },
      "source": [
        "## Part 2. Data preprocessing, model training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-b656a4266174b009",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "mo9TAQRt-_Qb"
      },
      "source": [
        "### 1. Reading the data\n",
        "Today we work with the [dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29), describing different cars for multiclass ($k=4$) classification problem. The data is available below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "mk4Fcrx6-_Qd"
      },
      "outputs": [],
      "source": [
        "# If on colab, uncomment the following lines\n",
        "\n",
        "#! wget https://raw.githubusercontent.com/girafe-ai/ml-course/22f_made/homeworks/lab01_ml_pipeline/car_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-eebac6bfdf73d0bc",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "lrLkOjPD-_Qf",
        "outputId": "82d37644-5593-414e-9aad-890cb837e944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(846, 19) (846,)\n",
            "(549, 19) (549,) (297, 19) (297,)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dataset = pd.read_csv('car_data.csv', delimiter=',', header=None).values\n",
        "data = dataset[:, :-1].astype(int)\n",
        "target = dataset[:, -1]\n",
        "\n",
        "print(data.shape, target.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.35)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-88b1a0f688568f2c",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "XkeMjcDK-_Qh"
      },
      "source": [
        "To get some insights about the dataset, `pandas` might be used. The `train` part is transformed to `pd.DataFrame` below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-98e7d91d77d65fcf",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "E2dxAkCJ-_Qj"
      },
      "source": [
        "Methods `describe` and `info` deliver some useful information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "XI0_P4DT-_Qi",
        "outputId": "c5158930-bc71-4f3a-e834-e40e457f33f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     0    1   2    3    4   5   6    7   8   9    10   11   12   13  14  15  \\\n",
              "0    72   92  39   91  191  62   8  176  37  21  137  196  466  151  67   3   \n",
              "1   331  104  52   96  188  59   9  188  35  22  161  206  530  205  67  11   \n",
              "2   582  108  53  104  181  56  11  220  31  25  167  226  712  214  72  15   \n",
              "3   611   90  49   85  141  57  11  159  43  20  167  173  365  186  75   1   \n",
              "4   562  113  53   93  197  62  11  216  31  24  165  221  688  196  72   6   \n",
              "5   732   85  45   73  167  69   8  143  46  18  148  173  307  176  71   2   \n",
              "6   344   99  55  101  219  68  10  224  30  25  178  228  737  213  74  11   \n",
              "7   188  101  51  105  212  68  10  209  32  24  162  222  653  224  73   5   \n",
              "8   646   90  38   79  185  69   6  160  40  20  130  178  393  133  66   2   \n",
              "9   356   91  45   76  171  69   7  150  44  19  144  170  340  179  69  12   \n",
              "10  355   93  42   88  188  62  10  183  36  21  141  208  504  168  70   3   \n",
              "11  752  100  36   73  199  73   6  162  40  20  127  189  401  125  72   6   \n",
              "12  600   98  46   77  199  71   7  166  39  20  150  184  422  180  69  13   \n",
              "13  628   98  51   84  207  72   7  184  35  21  161  199  520  198  72   9   \n",
              "14  678   86  45   70  122  56   7  148  45  19  144  170  324  186  84   9   \n",
              "\n",
              "    16   17   18  \n",
              "0   23  192  200  \n",
              "1    8  193  200  \n",
              "2   18  189  199  \n",
              "3   11  182  192  \n",
              "4   25  188  199  \n",
              "5    0  190  199  \n",
              "6   20  187  196  \n",
              "7   23  186  195  \n",
              "8   14  198  205  \n",
              "9    1  195  201  \n",
              "10  12  189  197  \n",
              "11  19  200  204  \n",
              "12   9  200  203  \n",
              "13  11  196  199  \n",
              "14   5  180  183  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>72</td>\n",
              "      <td>92</td>\n",
              "      <td>39</td>\n",
              "      <td>91</td>\n",
              "      <td>191</td>\n",
              "      <td>62</td>\n",
              "      <td>8</td>\n",
              "      <td>176</td>\n",
              "      <td>37</td>\n",
              "      <td>21</td>\n",
              "      <td>137</td>\n",
              "      <td>196</td>\n",
              "      <td>466</td>\n",
              "      <td>151</td>\n",
              "      <td>67</td>\n",
              "      <td>3</td>\n",
              "      <td>23</td>\n",
              "      <td>192</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>331</td>\n",
              "      <td>104</td>\n",
              "      <td>52</td>\n",
              "      <td>96</td>\n",
              "      <td>188</td>\n",
              "      <td>59</td>\n",
              "      <td>9</td>\n",
              "      <td>188</td>\n",
              "      <td>35</td>\n",
              "      <td>22</td>\n",
              "      <td>161</td>\n",
              "      <td>206</td>\n",
              "      <td>530</td>\n",
              "      <td>205</td>\n",
              "      <td>67</td>\n",
              "      <td>11</td>\n",
              "      <td>8</td>\n",
              "      <td>193</td>\n",
              "      <td>200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>582</td>\n",
              "      <td>108</td>\n",
              "      <td>53</td>\n",
              "      <td>104</td>\n",
              "      <td>181</td>\n",
              "      <td>56</td>\n",
              "      <td>11</td>\n",
              "      <td>220</td>\n",
              "      <td>31</td>\n",
              "      <td>25</td>\n",
              "      <td>167</td>\n",
              "      <td>226</td>\n",
              "      <td>712</td>\n",
              "      <td>214</td>\n",
              "      <td>72</td>\n",
              "      <td>15</td>\n",
              "      <td>18</td>\n",
              "      <td>189</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>611</td>\n",
              "      <td>90</td>\n",
              "      <td>49</td>\n",
              "      <td>85</td>\n",
              "      <td>141</td>\n",
              "      <td>57</td>\n",
              "      <td>11</td>\n",
              "      <td>159</td>\n",
              "      <td>43</td>\n",
              "      <td>20</td>\n",
              "      <td>167</td>\n",
              "      <td>173</td>\n",
              "      <td>365</td>\n",
              "      <td>186</td>\n",
              "      <td>75</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>182</td>\n",
              "      <td>192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>562</td>\n",
              "      <td>113</td>\n",
              "      <td>53</td>\n",
              "      <td>93</td>\n",
              "      <td>197</td>\n",
              "      <td>62</td>\n",
              "      <td>11</td>\n",
              "      <td>216</td>\n",
              "      <td>31</td>\n",
              "      <td>24</td>\n",
              "      <td>165</td>\n",
              "      <td>221</td>\n",
              "      <td>688</td>\n",
              "      <td>196</td>\n",
              "      <td>72</td>\n",
              "      <td>6</td>\n",
              "      <td>25</td>\n",
              "      <td>188</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>732</td>\n",
              "      <td>85</td>\n",
              "      <td>45</td>\n",
              "      <td>73</td>\n",
              "      <td>167</td>\n",
              "      <td>69</td>\n",
              "      <td>8</td>\n",
              "      <td>143</td>\n",
              "      <td>46</td>\n",
              "      <td>18</td>\n",
              "      <td>148</td>\n",
              "      <td>173</td>\n",
              "      <td>307</td>\n",
              "      <td>176</td>\n",
              "      <td>71</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>190</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>344</td>\n",
              "      <td>99</td>\n",
              "      <td>55</td>\n",
              "      <td>101</td>\n",
              "      <td>219</td>\n",
              "      <td>68</td>\n",
              "      <td>10</td>\n",
              "      <td>224</td>\n",
              "      <td>30</td>\n",
              "      <td>25</td>\n",
              "      <td>178</td>\n",
              "      <td>228</td>\n",
              "      <td>737</td>\n",
              "      <td>213</td>\n",
              "      <td>74</td>\n",
              "      <td>11</td>\n",
              "      <td>20</td>\n",
              "      <td>187</td>\n",
              "      <td>196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>188</td>\n",
              "      <td>101</td>\n",
              "      <td>51</td>\n",
              "      <td>105</td>\n",
              "      <td>212</td>\n",
              "      <td>68</td>\n",
              "      <td>10</td>\n",
              "      <td>209</td>\n",
              "      <td>32</td>\n",
              "      <td>24</td>\n",
              "      <td>162</td>\n",
              "      <td>222</td>\n",
              "      <td>653</td>\n",
              "      <td>224</td>\n",
              "      <td>73</td>\n",
              "      <td>5</td>\n",
              "      <td>23</td>\n",
              "      <td>186</td>\n",
              "      <td>195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>646</td>\n",
              "      <td>90</td>\n",
              "      <td>38</td>\n",
              "      <td>79</td>\n",
              "      <td>185</td>\n",
              "      <td>69</td>\n",
              "      <td>6</td>\n",
              "      <td>160</td>\n",
              "      <td>40</td>\n",
              "      <td>20</td>\n",
              "      <td>130</td>\n",
              "      <td>178</td>\n",
              "      <td>393</td>\n",
              "      <td>133</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>198</td>\n",
              "      <td>205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>356</td>\n",
              "      <td>91</td>\n",
              "      <td>45</td>\n",
              "      <td>76</td>\n",
              "      <td>171</td>\n",
              "      <td>69</td>\n",
              "      <td>7</td>\n",
              "      <td>150</td>\n",
              "      <td>44</td>\n",
              "      <td>19</td>\n",
              "      <td>144</td>\n",
              "      <td>170</td>\n",
              "      <td>340</td>\n",
              "      <td>179</td>\n",
              "      <td>69</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>195</td>\n",
              "      <td>201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>355</td>\n",
              "      <td>93</td>\n",
              "      <td>42</td>\n",
              "      <td>88</td>\n",
              "      <td>188</td>\n",
              "      <td>62</td>\n",
              "      <td>10</td>\n",
              "      <td>183</td>\n",
              "      <td>36</td>\n",
              "      <td>21</td>\n",
              "      <td>141</td>\n",
              "      <td>208</td>\n",
              "      <td>504</td>\n",
              "      <td>168</td>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>189</td>\n",
              "      <td>197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>752</td>\n",
              "      <td>100</td>\n",
              "      <td>36</td>\n",
              "      <td>73</td>\n",
              "      <td>199</td>\n",
              "      <td>73</td>\n",
              "      <td>6</td>\n",
              "      <td>162</td>\n",
              "      <td>40</td>\n",
              "      <td>20</td>\n",
              "      <td>127</td>\n",
              "      <td>189</td>\n",
              "      <td>401</td>\n",
              "      <td>125</td>\n",
              "      <td>72</td>\n",
              "      <td>6</td>\n",
              "      <td>19</td>\n",
              "      <td>200</td>\n",
              "      <td>204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>600</td>\n",
              "      <td>98</td>\n",
              "      <td>46</td>\n",
              "      <td>77</td>\n",
              "      <td>199</td>\n",
              "      <td>71</td>\n",
              "      <td>7</td>\n",
              "      <td>166</td>\n",
              "      <td>39</td>\n",
              "      <td>20</td>\n",
              "      <td>150</td>\n",
              "      <td>184</td>\n",
              "      <td>422</td>\n",
              "      <td>180</td>\n",
              "      <td>69</td>\n",
              "      <td>13</td>\n",
              "      <td>9</td>\n",
              "      <td>200</td>\n",
              "      <td>203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>628</td>\n",
              "      <td>98</td>\n",
              "      <td>51</td>\n",
              "      <td>84</td>\n",
              "      <td>207</td>\n",
              "      <td>72</td>\n",
              "      <td>7</td>\n",
              "      <td>184</td>\n",
              "      <td>35</td>\n",
              "      <td>21</td>\n",
              "      <td>161</td>\n",
              "      <td>199</td>\n",
              "      <td>520</td>\n",
              "      <td>198</td>\n",
              "      <td>72</td>\n",
              "      <td>9</td>\n",
              "      <td>11</td>\n",
              "      <td>196</td>\n",
              "      <td>199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>678</td>\n",
              "      <td>86</td>\n",
              "      <td>45</td>\n",
              "      <td>70</td>\n",
              "      <td>122</td>\n",
              "      <td>56</td>\n",
              "      <td>7</td>\n",
              "      <td>148</td>\n",
              "      <td>45</td>\n",
              "      <td>19</td>\n",
              "      <td>144</td>\n",
              "      <td>170</td>\n",
              "      <td>324</td>\n",
              "      <td>186</td>\n",
              "      <td>84</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>180</td>\n",
              "      <td>183</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 281
        }
      ],
      "source": [
        "X_train_pd = pd.DataFrame(X_train)\n",
        "\n",
        "# First 15 rows of our dataset.\n",
        "X_train_pd.head(15)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#y_train.shape\n",
        "X_train_pd[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJlbXZatBas1",
        "outputId": "fb6ff262-f0be-4a64-c44e-4ca68d498b50"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       92\n",
              "1      104\n",
              "2      108\n",
              "3       90\n",
              "4      113\n",
              "      ... \n",
              "544     98\n",
              "545     88\n",
              "546     98\n",
              "547     89\n",
              "548     93\n",
              "Name: 1, Length: 549, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {
        "id": "uCehTlik-_Ql",
        "outputId": "d0f6e899-405f-4189-fd30-289aa0204a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               0           1           2           3           4           5   \\\n",
              "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
              "mean   430.103825   93.196721   44.484517   81.156648  168.000000   61.870674   \n",
              "std    245.516055    8.060306    6.136988   15.541945   34.509969    8.432625   \n",
              "min      1.000000   73.000000   33.000000   40.000000  104.000000   48.000000   \n",
              "25%    218.000000   87.000000   39.000000   70.000000  140.000000   57.000000   \n",
              "50%    435.000000   92.000000   44.000000   78.000000  164.000000   61.000000   \n",
              "75%    640.000000   98.000000   49.000000   94.000000  194.000000   65.000000   \n",
              "max    843.000000  119.000000   59.000000  110.000000  333.000000  138.000000   \n",
              "\n",
              "               6           7           8           9           10          11  \\\n",
              "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
              "mean     8.657559  166.774135   41.336976   20.417122  147.278689  186.714026   \n",
              "std      5.118597   31.911774    7.657430    2.484800   14.529630   30.201274   \n",
              "min      2.000000  112.000000   26.000000   17.000000  118.000000  130.000000   \n",
              "25%      7.000000  146.000000   34.000000   19.000000  136.000000  167.000000   \n",
              "50%      8.000000  155.000000   43.000000   19.000000  145.000000  176.000000   \n",
              "75%     10.000000  193.000000   46.000000   22.000000  158.000000  214.000000   \n",
              "max     55.000000  262.000000   61.000000   28.000000  186.000000  320.000000   \n",
              "\n",
              "               12          13          14          15          16          17  \\\n",
              "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
              "mean   428.546448  172.559199   72.533698    6.358834   12.535519  188.939891   \n",
              "std    167.938554   31.866262    7.762514    4.948698    8.753368    6.205280   \n",
              "min    184.000000  109.000000   59.000000    0.000000    0.000000  176.000000   \n",
              "25%    317.000000  146.000000   68.000000    2.000000    5.000000  184.000000   \n",
              "50%    358.000000  172.000000   72.000000    6.000000   11.000000  188.000000   \n",
              "75%    567.000000  193.000000   75.000000    9.000000   19.000000  193.000000   \n",
              "max    987.000000  268.000000  135.000000   22.000000   41.000000  204.000000   \n",
              "\n",
              "               18  \n",
              "count  549.000000  \n",
              "mean   195.712204  \n",
              "std      7.292698  \n",
              "min    181.000000  \n",
              "25%    191.000000  \n",
              "50%    197.000000  \n",
              "75%    201.000000  \n",
              "max    211.000000  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "      <td>549.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>430.103825</td>\n",
              "      <td>93.196721</td>\n",
              "      <td>44.484517</td>\n",
              "      <td>81.156648</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>61.870674</td>\n",
              "      <td>8.657559</td>\n",
              "      <td>166.774135</td>\n",
              "      <td>41.336976</td>\n",
              "      <td>20.417122</td>\n",
              "      <td>147.278689</td>\n",
              "      <td>186.714026</td>\n",
              "      <td>428.546448</td>\n",
              "      <td>172.559199</td>\n",
              "      <td>72.533698</td>\n",
              "      <td>6.358834</td>\n",
              "      <td>12.535519</td>\n",
              "      <td>188.939891</td>\n",
              "      <td>195.712204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>245.516055</td>\n",
              "      <td>8.060306</td>\n",
              "      <td>6.136988</td>\n",
              "      <td>15.541945</td>\n",
              "      <td>34.509969</td>\n",
              "      <td>8.432625</td>\n",
              "      <td>5.118597</td>\n",
              "      <td>31.911774</td>\n",
              "      <td>7.657430</td>\n",
              "      <td>2.484800</td>\n",
              "      <td>14.529630</td>\n",
              "      <td>30.201274</td>\n",
              "      <td>167.938554</td>\n",
              "      <td>31.866262</td>\n",
              "      <td>7.762514</td>\n",
              "      <td>4.948698</td>\n",
              "      <td>8.753368</td>\n",
              "      <td>6.205280</td>\n",
              "      <td>7.292698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>112.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>130.000000</td>\n",
              "      <td>184.000000</td>\n",
              "      <td>109.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>181.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>218.000000</td>\n",
              "      <td>87.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>140.000000</td>\n",
              "      <td>57.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>136.000000</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>317.000000</td>\n",
              "      <td>146.000000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>184.000000</td>\n",
              "      <td>191.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>435.000000</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>164.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>155.000000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>145.000000</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>358.000000</td>\n",
              "      <td>172.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>197.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>640.000000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>49.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>194.000000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>193.000000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>158.000000</td>\n",
              "      <td>214.000000</td>\n",
              "      <td>567.000000</td>\n",
              "      <td>193.000000</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>193.000000</td>\n",
              "      <td>201.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>843.000000</td>\n",
              "      <td>119.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>333.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>262.000000</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>186.000000</td>\n",
              "      <td>320.000000</td>\n",
              "      <td>987.000000</td>\n",
              "      <td>268.000000</td>\n",
              "      <td>135.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>204.000000</td>\n",
              "      <td>211.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 283
        }
      ],
      "source": [
        "X_train_pd.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "o_m4Hjzz-_Qm",
        "outputId": "85161d32-4bc5-4cc4-b7bd-7a2e4d3e9e2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 549 entries, 0 to 548\n",
            "Data columns (total 19 columns):\n",
            " #   Column  Non-Null Count  Dtype\n",
            "---  ------  --------------  -----\n",
            " 0   0       549 non-null    int64\n",
            " 1   1       549 non-null    int64\n",
            " 2   2       549 non-null    int64\n",
            " 3   3       549 non-null    int64\n",
            " 4   4       549 non-null    int64\n",
            " 5   5       549 non-null    int64\n",
            " 6   6       549 non-null    int64\n",
            " 7   7       549 non-null    int64\n",
            " 8   8       549 non-null    int64\n",
            " 9   9       549 non-null    int64\n",
            " 10  10      549 non-null    int64\n",
            " 11  11      549 non-null    int64\n",
            " 12  12      549 non-null    int64\n",
            " 13  13      549 non-null    int64\n",
            " 14  14      549 non-null    int64\n",
            " 15  15      549 non-null    int64\n",
            " 16  16      549 non-null    int64\n",
            " 17  17      549 non-null    int64\n",
            " 18  18      549 non-null    int64\n",
            "dtypes: int64(19)\n",
            "memory usage: 81.6 KB\n"
          ]
        }
      ],
      "source": [
        "X_train_pd.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-be844269be69c387",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "CmcnuoqB-_Qn"
      },
      "source": [
        "### 2. Machine Learning pipeline\n",
        "Here you are supposed to perform the desired transformations. Please, explain your results briefly after each task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5M4vFWK-_Qo"
      },
      "source": [
        "#### 2.0. Data preprocessing\n",
        "* Make some transformations of the dataset (if necessary). Briefly explain the transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-a1514aa189a49fca",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "8qt542q8-_Qo"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "#X_train_s = X_train_pd[0:297]\n",
        "#X_train_pd[:,1]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB_TrNR8-_Qp"
      },
      "source": [
        "#### 2.1. Basic logistic regression\n",
        "* Find optimal hyperparameters for logistic regression with cross-validation on the `train` data (small grid/random search is enough, no need to find the *best* parameters).\n",
        "\n",
        "* Estimate the model quality with `f1` and `accuracy` scores.\n",
        "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
        "\n",
        "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` `tol=1e-3` and ` max_iter=500`.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-1dd5ad5d0845cbbb",
          "locked": false,
          "points": 5,
          "schema_version": 2,
          "solution": true
        },
        "id": "hz6q2cgD-_Qp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "5e1cae76-058a-4201-d6b1-59227b0e75c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/jctuesta/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Input \u001b[0;32mIn [296]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[::,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#auc = metrics.roc_auc_score(y_test, y_pred_proba)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mskplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_roc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scikitplot/metrics.py:414\u001b[0m, in \u001b[0;36mplot_roc\u001b[0;34m(y_true, y_probas, title, plot_micro, plot_macro, classes_to_plot, ax, figsize, cmap, title_fontsize, text_fontsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m indices_to_plot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39min1d(classes, classes_to_plot)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, to_plot \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indices_to_plot):\n\u001b[0;32m--> 414\u001b[0m     fpr_dict[i], tpr_dict[i], _ \u001b[38;5;241m=\u001b[39m roc_curve(y_true, \u001b[43mprobas\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m    415\u001b[0m                                             pos_label\u001b[38;5;241m=\u001b[39mclasses[i])\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m to_plot:\n\u001b[1;32m    417\u001b[0m         roc_auc \u001b[38;5;241m=\u001b[39m auc(fpr_dict[i], tpr_dict[i])\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "import scikitplot as skplt\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model = LogisticRegression(solver='saga',multi_class='multinomial',max_iter=500,tol=1e-3)\n",
        "model.fit(X_train, y_train)\n",
        "lr_probs = model.predict_proba(X_train)[::,1]\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "a = classification_report(y_test,y_pred)\n",
        "a\n",
        "\n",
        "\n",
        "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
        "#fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "#auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "skplt.metrics.plot_roc(X_test, lr_probs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy\", metrics.accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qafly96iHed",
        "outputId": "ca743312-27d5-4204-ddfe-f0dc39664bd7"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.6835016835016835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "x-L6277GiSz8",
        "outputId": "1f4d51a7-0376-45cd-d444-13db1bdf3268"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Input \u001b[0;32mIn [293]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[::,\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m fpr, tpr, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroc_curve\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43my_pred_proba\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m auc \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mroc_auc_score(y_test, y_pred_proba)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(fpr,tpr,label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata 1, auc=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(auc))\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:981\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroc_curve\u001b[39m(\n\u001b[1;32m    893\u001b[0m     y_true, y_score, \u001b[38;5;241m*\u001b[39m, pos_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, drop_intermediate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    894\u001b[0m ):\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[1;32m    897\u001b[0m \u001b[38;5;124;03m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    979\u001b[0m \n\u001b[1;32m    980\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     fps, tps, thresholds \u001b[38;5;241m=\u001b[39m \u001b[43m_binary_clf_curve\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;66;03m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;66;03m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m drop_intermediate \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fps) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:740\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    738\u001b[0m y_type \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pos_label \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m format is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(y_type))\n\u001b[1;32m    742\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m    743\u001b[0m y_true \u001b[38;5;241m=\u001b[39m column_or_1d(y_true)\n",
            "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov6Y4g7t-_Qq"
      },
      "outputs": [],
      "source": [
        "# You might use this command to install scikit-plot. \n",
        "# Warning, if you a running locally, don't call pip from within jupyter, call it from terminal in the corresponding \n",
        "# virtual environment instead\n",
        "\n",
        "# ! pip install scikit-plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCu75gro-_Qq"
      },
      "source": [
        "#### 2.2. PCA: explained variance plot\n",
        "* Apply the PCA to the train part of the data. Build the explaided variance plot. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-c6c614740bce090e",
          "locked": false,
          "points": 10,
          "schema_version": 2,
          "solution": true
        },
        "id": "pe1bQvuP-_Qr"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE\n",
        "  \n",
        " pca = decomposition.PCA()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-0c1fe666f52fe53c",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "pMM4_0nM-_Qr"
      },
      "source": [
        "#### 2.3. PCA trasformation\n",
        "* Select the appropriate number of components. Briefly explain your choice. Should you normalize the data?\n",
        "\n",
        "*Use `fit` and `transform` methods to transform the `train` and `test` parts.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-96ab18d96473ef71",
          "locked": false,
          "points": 5,
          "schema_version": 2,
          "solution": true
        },
        "id": "GZsnegGa-_Qr"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhsJF-yN-_Qs"
      },
      "source": [
        "**Note: From this point `sklearn` [Pipeline](https://scikit-learn.org/stable/modules/compose.html) might be useful to perform transformations on the data. Refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more information.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-d28b58a35c94e988",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "jK-guN8W-_Qs"
      },
      "source": [
        "#### 2.4. Logistic regression on PCA-preprocessed data.\n",
        "* Find optimal hyperparameters for logistic regression with cross-validation on the transformed by PCA `train` data.\n",
        "\n",
        "* Estimate the model quality with `f1` and `accuracy` scores.\n",
        "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
        "\n",
        "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` and `tol=1e-3`*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-12d53ea45258fa82",
          "locked": false,
          "points": 5,
          "schema_version": 2,
          "solution": true
        },
        "id": "KvrtzDjS-_Qt"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-4fbf16c64076e139",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "4NhjfyLJ-_Qt"
      },
      "source": [
        "#### 2.5. Decision tree\n",
        "* Now train a desicion tree on the same data. Find optimal tree depth (`max_depth`) using cross-validation.\n",
        "\n",
        "* Measure the model quality using the same metrics you used above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-748ed20b51c67fab",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "s_vmcStO-_Qt"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-9eadd4d8a03ae67a",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "qFfTgYmJ-_Qu"
      },
      "source": [
        "#### 2.6. Bagging.\n",
        "Here starts the ensembling part.\n",
        "\n",
        "First we will use the __Bagging__ approach. Build an ensemble of $N$ algorithms varying N from $N_{min}=2$ to $N_{max}=100$ (with step 5).\n",
        "\n",
        "We will build two ensembles: of logistic regressions and of decision trees.\n",
        "\n",
        "*Comment: each ensemble should be constructed from models of the same family, so logistic regressions should not be mixed up with decision trees.*\n",
        "\n",
        "\n",
        "*Hint 1: To build a __Bagging__ ensebmle varying the ensemble size efficiently you might generate $N_{max}$ subsets of `train` data (of the same size as the original dataset) using bootstrap procedure once. Then you train a new instance of logistic regression/decision tree with optimal hyperparameters you estimated before on each subset (so you train it from scratch). Finally, to get an ensemble of $N$ models you average the $N$ out of $N_{max}$ models predictions.*\n",
        "\n",
        "*Hint 2: sklearn might help you with this taks. Some appropriate function/class might be out there.*\n",
        "\n",
        "* Plot `f1` and `accuracy` scores plots w.r.t. the size of the ensemble.\n",
        "\n",
        "* Briefly analyse the plot. What is the optimal number of algorithms? Explain your answer.\n",
        "\n",
        "* How do you think, are the hyperparameters for the decision trees you found in 2.5 optimal for trees used in ensemble? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-8fc95a2b206bdae1",
          "locked": false,
          "points": 35,
          "schema_version": 2,
          "solution": true
        },
        "id": "8V88NwYO-_Qu"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTufvwlm-_Qu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-241b7691ab44cbfb",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "4XYyiSGT-_Qv"
      },
      "source": [
        "#### 2.7. Random Forest\n",
        "Now we will work with the Random Forest (its `sklearn` implementation).\n",
        "\n",
        "* * Plot `f1` and `accuracy` scores plots w.r.t. the number of trees in Random Forest.\n",
        "\n",
        "* What is the optimal number of trees you've got? Is it different from the optimal number of logistic regressions/decision trees in 2.6? Explain the results briefly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-888755d0f3d91620",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "LtRXaJ1_-_Qv"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-99191c0852538d4d",
          "locked": true,
          "schema_version": 2,
          "solution": false
        },
        "id": "8lrA-XQ--_Qv"
      },
      "source": [
        "#### 2.8. Learning curve\n",
        "Your goal is to estimate, how does the model behaviour change with the increase of the `train` dataset size.\n",
        "\n",
        "* Split the training data into 10 equal (almost) parts. Then train the models from above (Logistic regression, Desicion Tree, Random Forest) with optimal hyperparameters you have selected on 1 part, 2 parts (combined, so the train size in increased by 2 times), 3 parts and so on.\n",
        "\n",
        "* Build a plot of `accuracy` and `f1` scores on `test` part, varying the `train` dataset size (so the axes will be score - dataset size.\n",
        "\n",
        "* Analyse the final plot. Can you make any conlusions using it? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-e39bc7e7dff61ff9",
          "locked": false,
          "points": 15,
          "schema_version": 2,
          "solution": true
        },
        "id": "fFeHpjhZ-_Qv"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Create Assignment",
    "kernelspec": {
      "display_name": "Py3 Research",
      "language": "python",
      "name": "py3_research_kernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}